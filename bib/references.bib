@article{Adebayo2024Explainable,
	title = {Explainable {AI} in Robotics: A Critical Review and Implementation Strategies for Transparent Decision-Making},
	volume = {5},
	issn = {30509718},
	shorttitle = {Explainable AI in Robotics},
	url = {https://www.multidisciplinaryfrontiers.com/search?q=FMR-2025-1-004&search=search},
	doi = {10.54660/.IJFMR.2024.5.1.26-32},
	abstract = {The rapid advancement of AI-driven robotic systems has introduced significant challenges related to transparency and trust, particularly in safety-critical applications. This review paper critically examines the current approaches to Explainable AI (xAI) in robotics, emphasizing the inherent trade-offs between performance and transparency. While high-performance AI models are essential for complex robotic tasks, their opacity often undermines trust and limits adoption. To address this, the paper proposes a comprehensive framework for implementing xAI in robotics, including strategies such as modular architecture, hybrid models, and human-centered design. The paper also discusses key design considerations and evaluation metrics that ensure a balance between interpretability and operational effectiveness. Finally, the paper reflects on the implications of these strategies for the future of robotics. It suggests avenues for further research to enhance the integration of xAI, aiming to create more trustworthy and reliable robotic systems.},
	language = {en},
	number = {01},
	journal = {Journal of Frontiers in Multidisciplinary Research},
	author = {Adebayo, Abiodun Sunday and Ajayi, Olanrewaju Oluwaseun and Chukwurah, Naomi},
	year = {2024},
	pages = {26--32},
	series = {Frontiers},
	keywords = {type:surveys, conceptual}
}
@misc{Chakraborti2020Emerging,
	title = {The Emerging Landscape of Explainable {AI} Planning and Decision Making},
	url = {https://arxiv.org/abs/2002.11697},
	doi = {10.48550/arXiv.2002.11697},
	abstract = {In this paper, we provide a comprehensive outline of the different threads of work in Explainable AI Planning (XAIP) that has emerged as a focus area in the last couple of years, and contrast that with earlier efforts in the Ô¨Åeld in terms of techniques, target users, and delivery mechanisms. We hope that the survey will provide guidance to new researchers in automated planning towards the role of explanations in the effective design of human-inthe-loop systems, as well as provide the established researcher with some perspective on the evolution of the exciting world of explainable planning.},
	language = {en},
	publisher = {arXiv},
	author = {Chakraborti, Tathagata and Sreedharan, Sarath and Kambhampati, Subbarao},
	month = feb,
	year = {2020},
  	series = {arXiv},
	keywords ={type:surveys, conceptual}
}
@inproceedings{Cruz2022Evaluating,
	title = {Evaluating Human-like Explanations for Robot Actions in Reinforcement Learning Scenarios},
	url = {https://ieeexplore.ieee.org/abstract/document/9981334},
	doi = {10.1109/IROS47612.2022.9981334},
	abstract = {Explainable artiÔ¨Åcial intelligence is a research Ô¨Åeld that tries to provide more transparency for autonomous intelligent systems. Explainability has been used, particularly in reinforcement learning and robotic scenarios, to better understand the robot decision-making process. Previous work, however, has been widely focused on providing technical explanations that can be better understood by AI practitioners than non-expert endusers. In this work, we make use of human-like explanations built from the probability of success to complete the goal that an autonomous robot shows after performing an action. These explanations are intended to be understood by people who have no or very little experience with artiÔ¨Åcial intelligence methods. This paper presents a user trial to study whether these explanations that focus on the probability an action has of succeeding in its goal constitute a suitable explanation for non-expert endusers. The results obtained show that non-expert participants rate robot explanations that focus on the probability of success higher and with less variance than technical explanations generated from Q-values, and also favor counterfactual explanations over standalone explanations.},
	language = {en},
	author = {Cruz, Francisco and Young, Charlotte and Dazeley, Richard and Vamplew, Peter},
	month = jul,
  	series = {IROS},
	keywords = {type:learning-based, simulation-centric, probabilistic},
	booktitle={2022 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages={894--901},
	year={2022},
	organization={Institute of Electrical and Electronics Engineers},
	publisher={IEEE}
}
@inproceedings{Das2021Explainable,
	title = {Explainable {AI} for Robot Failures: Generating Explanations that Improve User Assistance in Fault Recovery},
	shorttitle = {Explainable {AI} for Robot Failures},
	url = {https://dl.acm.org/doi/10.1145/3434073.3444657},
	doi = {10.1145/3434073.3444657},
	abstract = {With the growing capabilities of intelligent systems, the integration of robots in our everyday life is increasing. However, when interacting in such complex human environments, the occasional failure of robotic systems is inevitable. The field of explainable AI has sought to make complex-decision making systems more interpretable but most existing techniques target domain experts. On the contrary, in many failure cases, robots will require recovery assistance from non-expert users. In this work, we introduce a new type of explanation, Eùëíùëüùëü , that explains the cause of an unexpected failure during an agent‚Äôs plan execution to non-experts. In order for Eùëíùëüùëü to be meaningful, we investigate what types of information within a set of hand-scripted explanations are most helpful to non-experts for failure and solution identification. Additionally, we investigate how such explanations can be autonomously generated, extending an existing encoder-decoder model, and generalized across environments. We investigate such questions in the context of a robot performing a pick-and-place manipulation task in the home environment. Our results show that explanations capturing the context of a failure and history of past actions, are the most effective for failure and solution identification among non-experts. Furthermore, through a second user evaluation, we verify that our model-generated explanations can generalize to an unseen office environment, and are just as effective as the hand-scripted explanations.},
	language = {en},
	booktitle = {Proceedings of the 2021 {ACM}/{IEEE} International Conference on Human-Robot Interaction},
	author = {Das, Devleena and Banerjee, Siddhartha and Chernova, Sonia},
	month = mar,
	year = {2021},
	keywords = {type:user-centric, error-diagnosis},
	pages = {351--360},
	series = {ACM},
	organization={Association for Computing Machinery},
	publisher = {ACM}
}
@misc{Halilovic2024Towards,
	title = {Towards Probabilistic Planning of Explanations for Robot Navigation},
	url = {http://arxiv.org/abs/2411.05022},
	doi = {10.48550/arXiv.2411.05022},
	abstract = {In robotics, ensuring that autonomous systems are comprehensible and accountable to users is essential for effective human-robot interaction. This paper introduces a novel approach that integrates user-centered design principles directly into the core of robot path planning processes. We propose a probabilistic framework for automated planning of explanations for robot navigation, where the preferences of different users regarding explanations are probabilistically modeled to tailor the stochasticity of the real-world human-robot interaction and the communication of a robot‚Äôs decisions and actions towards humans. This approach aims to enhance the transparency of robot path planning and adapt to diverse user explanation needs by anticipating the types of explanations that will satisfy individual users.},
	language = {en},
	publisher = {arXiv},
	author = {Halilovic, Amar and Krivic, Senka},
	month = oct,
	year = {2024},
  	series = {arXiv},
	keywords = {type:xaip, probabilistic, algorithmic}
}
@misc{Matarese2021User,
	title = {A User-Centred Framework for Explainable ArtiÔ¨Åcial Intelligence in Human-Robot Interaction},
	url = {https://arxiv.org/abs/2109.12912},
	doi = {10.48550/arXiv.2109.12912},
	abstract = {State of the art ArtiÔ¨Åcial Intelligence (AI) techniques have reached an impressive complexity. Consequently, researchers are discovering more and more methods to use them in realworld applications. However, the complexity of such systems requires the introduction of methods that make those transparent to the human user. The AI community is trying to overcome the problem by introducing the Explainable AI (XAI) Ô¨Åeld, which is tentative to make AI algorithms less opaque. However, in recent years, it became clearer that XAI is much more than a computer science problem: since it is about communication, XAI is also a Human-Agent Interaction problem. Moreover, AI came out of the laboratories to be used in real life. This implies the need for XAI solutions tailored to nonexpert users. Hence, we propose a user-centred framework for XAI that focuses on its social-interactive aspect taking inspiration from cognitive and social sciences‚Äô theories and Ô¨Åndings. The framework aims to provide a structure for interactive XAI solutions thought for non-expert users.},
	language = {en},
	author = {Matarese, Marco and Rea, Francesco and Sciutti, Alessandra},
	publisher = {arXiv},
	month = sep,
	year = {2021},
	series = {arXiv},
	keywords = {type:user-centric, causal}
}
@misc{Sagar2024Trustworthy,
	title = {Trustworthy Conceptual Explanations for Neural Networks in Robot Decision-Making},
	url = {http://arxiv.org/abs/2409.10733},
	doi = {10.48550/arXiv.2409.10733},
	abstract = {Black box neural networks are an indispensable part of modern robots. Nevertheless, deploying such high-stakes systems in real-world scenarios poses significant challenges when the stakeholders, such as engineers and legislative bodies, lack insights into the neural networks' decision-making process. Presently, explainable AI is primarily tailored to natural language processing and computer vision, falling short in two critical aspects when applied in robots: grounding in decision-making tasks and the ability to assess trustworthiness of their explanations. In this paper, we introduce a trustworthy explainable robotics technique based on human-interpretable, high-level concepts that attribute to the decisions made by the neural network. Our proposed technique provides explanations with associated uncertainty scores by matching neural network's activations with human-interpretable visualizations. To validate our approach, we conducted a series of experiments with various simulated and real-world robot decision-making models, demonstrating the effectiveness of the proposed approach as a post-hoc, human-friendly robot learning diagnostic tool.},
	language = {en},
	publisher = {arXiv},
	author = {Sagar, Som and Taparia, Aditya and Mankodiya, Harsh and Bidare, Pranav and Zhou, Yifan and Senanayake, Ransalu},
	month = sep,
	year = {2024},
	series = {arXiv},
	keywords = {type:concept-driven, simulation-centric, algorithmic, real-robot}
}
@article{Sakai2021Framework,
	title = {A Framework of Explanation Generation toward Reliable Autonomous Robots},
	url = {https://www.tandfonline.com/doi/full/10.1080/01691864.2021.1946423},
	doi = {10.1080/01691864.2021.1946423},
	abstract = {To realize autonomous collaborative robots, it is important to increase the trust that users have in them. Toward this goal, this paper proposes an algorithm which endows an autonomous agent with the ability to explain the transition from the current state to the target state in a Markov decision process (MDP). According to cognitive science, to generate an explanation that is acceptable to humans, it is important to present the minimum information necessary to suÔ¨Éciently understand an event. To meet this requirement, this study proposes a framework for identifying important elements in the decision-making process using a prediction model for the world and generating explanations based on these elements. To verify the ability of the proposed method to generate explanations, we conducted an experiment using a grid environment. It was inferred from the result of a simulation experiment that the explanation generated using the proposed method was composed of the minimum elements important for understanding the transition from the current state to the target state. Furthermore, subject experiments showed that the generated explanation was a good summary of the process of state transition, and that a high evaluation was obtained for the explanation of the reason for an action.},
	language = {en},
	author = {Sakai, Tatsuya and Miyazawa, Kazuki and Horii, Takato and Nagai, Takayuki},
	month = may,
	journal={Advanced Robotics},
	volume={35},
	number={17},
	pages={1054--1067},
	year={2021},
	publisher={Taylor & Francis},
	series = {Special},
	keywords = {type:mdp-based, simulation-centric, causal}
}
@article{Strange2022Recent,
	title={Self-explaining social robots: An explainable behavior generation architecture for human-robot interaction},
	url = {https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2022.866920/full},
	doi = {10.3389/frai.2022.866920},
	abstract = {In recent years, the ability of intelligent systems to be understood by developers and users has received growing attention. This holds in particular for social robots, which are supposed to act autonomously in the vicinity of human users and are known to raise peculiar, often unrealistic attributions and expectations. However, explainable models that, on the one hand, allow a robot to generate lively and autonomous behavior and, on the other, enable it to provide human-compatible explanations for this behavior are missing. In order to develop such a self-explaining autonomous social robot, we have equipped a robot with own needs that autonomously trigger intentions and proactive behavior, and form the basis for understandable self-explanations. Previous research has shown that undesirable robot behavior is rated more positively after receiving an explanation. We thus aim to equip a social robot with the capability to automatically generate verbal explanations of its own behavior, by tracing its internal decision-making routes. The goal is to generate social robot behavior in a way that is generally interpretable, and therefore explainable on a socio-behavioral level increasing users‚Äô understanding of the robot‚Äôs behavior. In this article, we present a social robot interaction architecture, designed to autonomously generate social behavior and self-explanations. We set out requirements for explainable behavior generation architectures and propose a socio-interactive framework for behavior explanations in social human-robot interactions that enables explaining and elaborating according to users‚Äô needs for explanation that emerge within an interaction. Consequently, we introduce an interactive explanation dialog Ô¨Çow concept that incorporates empirically validated explanation types. These concepts are realized within the interaction architecture of a social robot, and integrated with its dialog processing modules. We present the components of this interaction architecture and explain their integration to autonomously generate social behaviors as well as verbal self-explanations. Lastly, we report results from a qualitative evaluation of a working prototype in a laboratory setting, showing that (1) the robot is able to autonomously generate naturalistic social behavior, and (2) the robot is able to verbally self-explain its behavior to the user in line with users‚Äô requests.},
	language = {en},
	author={Stange, Sonja and Hassan, Teena and Schr{\"o}der, Florian and Konkol, Jacqueline and Kopp, Stefan},
	journal={Frontiers in Artificial Intelligence},
	volume={5},
	pages={866920},
	year={2022},
	publisher={Frontiers Media {SA}},
	series = {Frontiers},
	keywords = {type:social-interactive, real-robot}
}
@misc{Yu2023Explainable,	
	title = {Explainable Reinforcement Learning via a Causal World Model},
	url = {https://arxiv.org/abs/2305.02749},
	doi = {10.48550/arXiv.2305.02749},
	abstract = {Generating explanations for reinforcement learning (RL) is challenging as actions may produce longterm effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.},
	language = {en},
	publisher = {arXiv},
	author = {Yu, Zhongwei and Ruan, Jingqing and Xing, Dengpeng},
	month = may,
	year = {2023},
	series = {arXiv},
	keywords = {type:learning-based, simulation-based, causal}
}


